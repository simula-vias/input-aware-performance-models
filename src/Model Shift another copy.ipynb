{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for arrays\n",
    "import numpy as np\n",
    "\n",
    "# for dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "# high-level plots\n",
    "import seaborn as sns\n",
    "\n",
    "# statistics\n",
    "import scipy.stats as sc\n",
    "# hierarchical clustering, clusters\n",
    "from scipy.cluster.hierarchy import linkage, cut_tree, leaves_list\n",
    "from scipy import stats\n",
    "# statistical tests\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# machine learning library\n",
    "# Principal Component Analysis - determine new axis for representing data\n",
    "from sklearn.decomposition import PCA\n",
    "# Random Forests -> vote between decision trees\n",
    "# Gradient boosting -> instead of a vote, upgrade the same tree\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingClassifier\n",
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "# To add interactions in linear regressions models\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Elasticnet is an hybrid method between ridge and Lasso\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "# To separate the data into training and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Simple clustering (iterative steps)\n",
    "from sklearn.cluster import KMeans\n",
    "# get interactions of features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# gradient boosting trees\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# we use it to interact with the file system\n",
    "import os\n",
    "# compute time\n",
    "from time import time\n",
    "\n",
    "# statistics\n",
    "import scipy.stats as sc\n",
    "# hierarchical clustering, clusters\n",
    "from scipy.cluster.hierarchy import linkage, cut_tree, leaves_list\n",
    "from scipy import stats\n",
    "# statistical tests\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# no warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from common import load_data, split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_dir = \"../data/\"\n",
    "full_data = load_data(data_dir)\n",
    "\n",
    "data = full_data[\"data\"]\n",
    "inputs_perf = full_data[\"performance_properties\"]\n",
    "inputs_feat = full_data[\"features\"]\n",
    "inputs_categ = full_data[\"features_categorical\"]\n",
    "inputs_num = full_data[\"features_numerical\"]\n",
    "inputs_feat_cols = full_data[\"feature_columns\"]\n",
    "inputs_prop = full_data[\"input_properties\"]\n",
    "inputs_name = full_data[\"input_names\"]\n",
    "inputs_count = full_data[\"input_counts\"]\n",
    "\n",
    "random_seed = 100\n",
    "system = \"gcc\"\n",
    "train_data, test_data, _ , _ = split_data(\n",
    "    data, system, inputs_count, inputs_feat_cols, random_seed\n",
    ")\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "valid_training_percentages = [(k+1)/10 for k in range(9)] # 0.1 -> 0.9 included\n",
    "\n",
    "train = dict()\n",
    "val = dict()\n",
    "\n",
    "for dkey in data.keys():\n",
    "    soft, input_id = dkey\n",
    "    for p in valid_training_percentages:\n",
    "        X_train, X_test = train_test_split(data[soft, input_id], train_size=p)\n",
    "        train[soft, input_id, p] = X_train\n",
    "        val[soft, input_id, p] = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Shift (MS) is a transfer learning defined by Valov et al. \n",
    "First, it trains a performance model on the source input and predicts the performance distribution of the source input. \n",
    "Then, it trains a shifting function, predicting the performances of the target input based on the performances of the source. \n",
    "Finally, it applies the shifting function to the predictions of the source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MS:\n",
    "    \n",
    "    def __init__(self, ns, performance, nb_input_clusters = 4):\n",
    "        \n",
    "        # the name of the current software system \n",
    "        self.ns = ns\n",
    "        \n",
    "        # the performance to predict\n",
    "        self.performance = performance\n",
    "        \n",
    "        # see above\n",
    "        self.inputs_name = inputs_name[self.ns]\n",
    "        self.inputs_perf = inputs_perf[self.ns]\n",
    "        self.inputs_prop = inputs_prop[self.ns].drop(['name'], axis = 1)\n",
    "        \n",
    "        # the total number of inputs for this software system\n",
    "        self.nb_inputs = inputs_count[self.ns]\n",
    "        \n",
    "        # the data to work with - removing  other performance properties\n",
    "        self.data = dict()\n",
    "        for i in range(self.nb_inputs):\n",
    "            self.data[i] = data[self.ns, i].drop(np.setdiff1d(self.inputs_perf, [self.performance]), axis = 1)\n",
    "        \n",
    "        # random state , i.e. a seed to split the source and the target datasets\n",
    "        # by using the same set of configurations - needed to compare \n",
    "        # the performance of configurations used in the training set of the target \n",
    "        self.random_state = np.random.randint(0,1000)\n",
    "        \n",
    "        # the total number of configurations measured per input\n",
    "        self.nb_config = len(data[self.ns, 0].index)\n",
    "        \n",
    "        # compute groups of inputs thanks to input properties\n",
    "        clust_alg = KMeans(n_clusters = nb_input_clusters)\n",
    "        self.prop_dummies = pd.get_dummies(self.inputs_prop).fillna(0)\n",
    "        # we normalize the different properties so the clustering algorithm\n",
    "        # is not influenced by the scale of the properties\n",
    "        for col in self.prop_dummies.columns:\n",
    "            self.prop_dummies[col] = (self.prop_dummies[col]-np.mean(\n",
    "                self.prop_dummies[col]))/np.std(self.prop_dummies[col])\n",
    "        # computing the clusters/groups of inputs\n",
    "        clust_alg.fit(self.prop_dummies)\n",
    "        self.inputs_grps = clust_alg.predict(self.prop_dummies)\n",
    "    \n",
    "    def choose_best_source(self, method):\n",
    "        # random source input\n",
    "        if method == 'random':\n",
    "            output_id = self.target_id\n",
    "            # to avoid choosing the target as source, which would be unfair\n",
    "            max_iter = 0\n",
    "            while output_id == self.target_id and max_iter < 50:\n",
    "                output_id = np.random.randint(0, self.nb_inputs)\n",
    "                max_iter+=1\n",
    "        # random source input chosen in the same group of inputs\n",
    "        if method == 'same_input_grp':\n",
    "            # we randomly chose some inputs that we can use as source input for the given target\n",
    "            target_grp = self.inputs_grps[self.target_id]\n",
    "            target_grp_inputs = [i for i in range(len(self.inputs_grps)) if self.inputs_grps[i] == target_grp]\n",
    "            output_id = self.target_id\n",
    "            # to avoid choosing the target as source, which would be unfair\n",
    "            max_iter = 0\n",
    "            while output_id == self.target_id and max_iter < 50:\n",
    "                output_id = target_grp_inputs[np.random.randint(0, len(target_grp_inputs))]\n",
    "                max_iter+=1\n",
    "        else:\n",
    "            # select a set of potential source inputs\n",
    "            y = [k for k in range(self.nb_inputs)]\n",
    "            potential_sources, _ = train_test_split(y, train_size = self.nb_available_source_inputs)\n",
    "            \n",
    "            # select the source input that has a set of properties as close as possible of the target\n",
    "            if method =='closest_properties':\n",
    "                # the mean absolute error between the input properties of the sources and the target\n",
    "                diff_src_tgt = [mean_absolute_error(self.prop_dummies.iloc[self.target_id],\n",
    "                                                    self.prop_dummies.iloc[ps]) if ps != self.target_id else 100 for ps in potential_sources]\n",
    "                # we select the source for which the difference of properties is minimal\n",
    "                output_id = potential_sources[np.argmin(diff_src_tgt)]\n",
    "            \n",
    "            # best performance correlation between the potential source and the given target\n",
    "            if method == 'max_perf_corr':\n",
    "                target = self.data[self.target_id]\n",
    "                y_tgt = np.array(target[self.performance], dtype=float)\n",
    "                y_tgt_train, _ = train_test_split(y_tgt,\n",
    "                                                           train_size=self.train_size, \n",
    "                                                           random_state=self.random_state)\n",
    "                corr_src_tgt = []\n",
    "                \n",
    "                for ps in potential_sources:\n",
    "                    if ps != self.target_id:\n",
    "                        y_src = self.data[ps][self.performance]\n",
    "                        y_src_train, _ = train_test_split(y_src,\n",
    "                                                          train_size=self.train_size, \n",
    "                                                          random_state=self.random_state)\n",
    "                        corr_src_tgt.append(np.corrcoef(y_src_train, y_tgt_train)[0,1])\n",
    "                    else:\n",
    "                        corr_src_tgt.append(0)\n",
    "                # we select the source having the greater correlation with the target\n",
    "                # for the training set of configurations\n",
    "                output_id = potential_sources[np.argmax(corr_src_tgt)]\n",
    "                \n",
    "        return output_id\n",
    "    \n",
    "    \n",
    "    def learn(self, \n",
    "              target_id, \n",
    "              proportion_training_config, \n",
    "              method,\n",
    "              prop_available_source_inputs,\n",
    "              learning_algorithm = RandomForestRegressor, \n",
    "              shift_function = RandomForestRegressor):\n",
    "    \n",
    "        # the number of configurations used for the training of the transfer\n",
    "        self.train_size = int(proportion_training_config*self.nb_config)\n",
    "        \n",
    "        # the number of potential inputs that we can use as source\n",
    "        self.nb_available_source_inputs = int(prop_available_source_inputs*self.nb_inputs) \n",
    "        \n",
    "        # the target id\n",
    "        self.target_id = target_id\n",
    "        \n",
    "        # choose the source id\n",
    "        source_id = self.choose_best_source(method)\n",
    "\n",
    "        # we define the source input, and split it into train-test\n",
    "        source = self.data[source_id]\n",
    "        X_src = source.drop([self.performance], axis = 1)\n",
    "        y_src = np.array(source[self.performance], dtype=float)\n",
    "        X_src_train, X_src_test, y_src_train, y_src_test = train_test_split(X_src, \n",
    "                                                                            y_src, \n",
    "                                                                            train_size=self.train_size,\n",
    "                                                                            random_state=self.random_state)\n",
    "        \n",
    "        # We define the target input, and split it into train-test\n",
    "        target = self.data[self.target_id]\n",
    "        X_tgt = target.drop([self.performance], axis = 1)\n",
    "        y_tgt = np.array(target[self.performance], dtype=float)\n",
    "        X_tgt_train, X_tgt_test, y_tgt_train, y_tgt_test = train_test_split(X_tgt, \n",
    "                                                                            y_tgt, \n",
    "                                                                            train_size=self.train_size, \n",
    "                                                                            random_state=self.random_state)\n",
    "\n",
    "        # the learning algorithm, training on the source video\n",
    "        # X_src_train2, _, y_src_train2, _ = train_test_split(X_src, y_src, test_size=0.7)\n",
    "        lf = learning_algorithm()\n",
    "        lf.fit(X_src, y_src)\n",
    "        y_src_pred_test = np.array(lf.predict(X_src_test)).reshape(-1,1)\n",
    "\n",
    "        # the shift function, to transfer the prediction from the source to the target\n",
    "        shift = shift_function()\n",
    "        shift.fit(np.array(y_src_train).reshape(-1,1), y_tgt_train)\n",
    "        y_tgt_pred_test = shift.predict(y_src_test.reshape(-1,1))\n",
    "\n",
    "        # we return the mean average percentage error \n",
    "        # between the real values of y_test from target \n",
    "        # and the predictions shifted \n",
    "        # np.argmin(y_tgt_pred_test) to predict the best config\n",
    "        return mean_absolute_percentage_error(y_tgt_pred_test, y_tgt_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodejs ops\n",
      "poppler size\n",
      "poppler time\n",
      "xz size\n",
      "xz time\n",
      "x264 size\n",
      "x264 kbs\n",
      "x264 fps\n",
      "x264 etime\n",
      "x264 cpu\n",
      "gcc size\n",
      "gcc ctime\n",
      "gcc exec\n",
      "lingeling conflicts\n",
      "lingeling cps\n",
      "lingeling reductions\n",
      "sqlite q1\n",
      "sqlite q2\n",
      "sqlite q3\n",
      "sqlite q4\n",
      "sqlite q5\n",
      "sqlite q6\n",
      "sqlite q7\n",
      "sqlite q8\n",
      "sqlite q9\n",
      "sqlite q10\n",
      "sqlite q11\n",
      "sqlite q12\n",
      "sqlite q13\n",
      "sqlite q14\n",
      "sqlite q15\n",
      "imagemagick size\n",
      "imagemagick time\n"
     ]
    }
   ],
   "source": [
    "# defined above :  \n",
    "# name_systems = [\"nodejs\", \"poppler\", \"xz\", \"x264\", \"gcc\", \"lingeling\", \"sqlite\", \"imagemagick\"]\n",
    "\n",
    "prop_train_config = [0.1* k for k in range(1,10)]\n",
    "proportion_inputs = [0.1* k for k in range(1,10)]\n",
    "methods = ['random', 'same_input_grp', 'closest_properties', 'max_perf_corr']\n",
    "repetitions = 10\n",
    "\n",
    "for ns in name_systems:\n",
    "    for perf in inputs_perf[ns]:\n",
    "        \n",
    "        print(ns,perf)\n",
    "        \n",
    "        ms = MS(ns, perf)\n",
    "        \n",
    "        res = dict()\n",
    "\n",
    "        for ptc in prop_train_config:\n",
    "            for nbi in proportion_inputs:\n",
    "                for m in methods:\n",
    "                    for r in range(repetitions):\n",
    "                        # we randomly chose the index of the target input\n",
    "                        index_target = np.random.randint(inputs_count[ns])\n",
    "                        # start timer\n",
    "                        s = time()\n",
    "                        val = ms.learn(target_id = index_target, \n",
    "                                       proportion_training_config = ptc, \n",
    "                                       method = m,\n",
    "                                       prop_available_source_inputs = nbi)\n",
    "                        # end timer\n",
    "                        e = time()\n",
    "                        res[ptc, m, nbi, r] = (val, e-s)\n",
    "\n",
    "        final_results = []\n",
    "\n",
    "        for i in res.keys():\n",
    "            ptc, m, nbi, r = i\n",
    "            mape, eltime = res[i]\n",
    "            final_results.append((ptc, m, nbi, r, mape, eltime))\n",
    "\n",
    "        df = pd.DataFrame(final_results)\n",
    "        df.columns = ['proportion_train_config',  'source_selection_method', 'prop_inputs_source',\n",
    "                      'id repetition', 'mape', 'training_time']\n",
    "        df.to_csv('../results/'+ns+'/MS_'+perf+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined above :  \n",
    "# name_systems = [\"nodejs\", \"poppler\", \"xz\", \"x264\", \"gcc\", \"lingeling\", \"sqlite\", \"imagemagick\"]\n",
    "\n",
    "prop_train_config = [0.1* k for k in range(1,10)]\n",
    "proportion_inputs = [0.1* k for k in range(1,10)]\n",
    "methods = ['random', 'same_input_grp', 'closest_properties', 'max_perf_corr']\n",
    "repetitions = 10\n",
    "\n",
    "for ns in name_systems:\n",
    "    for perf in inputs_perf[ns]:\n",
    "        \n",
    "        print(ns,perf)\n",
    "        \n",
    "        ms = MS(ns, perf)\n",
    "        \n",
    "        res = dict()\n",
    "\n",
    "        for ptc in prop_train_config:\n",
    "            for nbi in proportion_inputs:\n",
    "                for m in methods:\n",
    "                    for r in range(repetitions):\n",
    "                        # we randomly chose the index of the target input\n",
    "                        index_target = np.random.randint(inputs_count[ns])\n",
    "                        # start timer\n",
    "                        s = time()\n",
    "                        val = ms.learn(target_id = index_target, \n",
    "                                       proportion_training_config = ptc, \n",
    "                                       method = m,\n",
    "                                       prop_available_source_inputs = nbi)\n",
    "                        # end timer\n",
    "                        e = time()\n",
    "                        res[ptc, m, nbi, r] = (val, e-s)\n",
    "\n",
    "        final_results = []\n",
    "\n",
    "        for i in res.keys():\n",
    "            ptc, m, nbi, r = i\n",
    "            mape, eltime = res[i]\n",
    "            final_results.append((ptc, m, nbi, r, mape, eltime))\n",
    "\n",
    "        df = pd.DataFrame(final_results)\n",
    "        df.columns = ['proportion_train_config',  'source_selection_method', 'prop_inputs_source',\n",
    "                      'id repetition', 'mape', 'training_time']\n",
    "        df.to_csv('../results/'+ns+'/MS_'+perf+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
